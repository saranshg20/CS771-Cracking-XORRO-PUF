{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_OpqLX1wFy3X"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CMQ5kZB_GS7Z"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('train.dat')\n",
    "data1 = np.genfromtxt('test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 0., ..., 1., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jzAkzTUIHIvv"
   },
   "outputs": [],
   "source": [
    "def binary_to_decimal(data, row):\n",
    "    binary1=''\n",
    "    binary2=''\n",
    "\n",
    "    for i in data[row][64:68]:\n",
    "        binary1 += str(int(i))\n",
    "\n",
    "    for j in data[row][68:72]:\n",
    "        binary2 += str(int(j))\n",
    "\n",
    "    p = int(binary1, 2)\n",
    "    q = int(binary2, 2)\n",
    "\n",
    "    return [p, q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BfrwLaC_HMZ-"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for row in range(60000):\n",
    "    challenge = np.append(np.append(data[row][0:64], binary_to_decimal(data, row)), data[row][-1])\n",
    "    train_data.append(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JD0gopnLHRzQ"
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for row in range(40000):\n",
    "    challenge = np.append(np.append(data1[row][0:64], binary_to_decimal(data1, row)), data1[row][-1])\n",
    "    test_data.append(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gGbdBPLDHT2A"
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8AOmKFQBHV0a"
   },
   "outputs": [],
   "source": [
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty((0, 65), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.append(a, np.array([np.delete(train_data[0], [64, 65])]), axis=0)\n",
    "a = np.append(a, np.array([np.delete(train_data[1], [64, 65])]), axis=0)\n",
    "a = np.append(a, np.array([np.delete(train_data[2], [64, 65])]), axis=0)\n",
    "a = np.append(a, np.array([np.delete(train_data[3], [64, 65])]), axis=0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ARixA82jHXux"
   },
   "outputs": [],
   "source": [
    "# def create_feature(X_train):\n",
    "#     X = []\n",
    "#     for challenge in X_train:\n",
    "#         temp = []\n",
    "#         for val in challenge:\n",
    "#             temp.append(1-val)\n",
    "#             temp.append(val)\n",
    "#     # temp.append(1)\n",
    "#         X.append(temp)\n",
    "\n",
    "#     X = np.array(X)\n",
    "#     return X\n",
    "\n",
    "# def create_feature(X_train):\n",
    "#     X = []\n",
    "#     for challenge in X_train:\n",
    "#         temp = []\n",
    "#         for val in challenge:\n",
    "#             if val == 1.0:\n",
    "#                 temp.append(val)\n",
    "#             elif val == 0.0:\n",
    "#                 temp.append(-1.0)\n",
    "#         temp.append(1)\n",
    "#         X.append(temp)\n",
    "\n",
    "#     X = np.array(X)\n",
    "#     return X\n",
    "\n",
    "def create_feature(data):\n",
    "    for challenge in data:\n",
    "        np.append(challenge, 1.0)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data1 = [] # when select bit is (p, q),\n",
    "trial_data2 = [] # when select bit is (q, p). here p<q\n",
    "\n",
    "for challenge in train_data:\n",
    "    if (list(challenge[64:66]) == [1.0, 3.0]):\n",
    "        trial_data1.append(challenge)\n",
    "    if (list(challenge[64:66]) == [3.0, 1.0]):\n",
    "        trial_data2.append(challenge)\n",
    "\n",
    "trial_data1 = np.array(trial_data1)\n",
    "trial_data2 = np.array(trial_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = []\n",
    "\n",
    "for challenge in train_data:\n",
    "    if (list(challenge[64:66]) == [0.0, 1.0]):\n",
    "        trial_data.append(challenge)\n",
    "    if (list(challenge[64:66]) == [1.0, 0.0]):\n",
    "        check_challenge = challenge\n",
    "        check_challenge[64:66] = [0.0, 1.0]\n",
    "        a = [list(item) for item in trial_data]\n",
    "        if list(check_challenge) not in a:\n",
    "            challenge[-1] = 1.0 - challenge[-1]\n",
    "            trial_data.append(challenge)\n",
    "\n",
    "trial_data = np.array(trial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = np.delete(trial_data, [64, 65], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(478, 65)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trial_data)\n",
    "np.shape(trial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(create_feature(trial_data[:, :-1]))\n",
    "# # np.shape(trial_data[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(trial_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trial_data, trial_data1, trial_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "E-9LX8HJH7NT"
   },
   "outputs": [],
   "source": [
    "# def fit(model, Z_train):\n",
    "#   # for select bit p and q\n",
    "#   for p in range(0, 15):\n",
    "#     for q in range(p+1, 16):\n",
    "# #       trial_data1 = [] # when select bit is (p, q),\n",
    "# #       trial_data2 = [] # when select bit is (q, p). here p<q\n",
    "\n",
    "# #       for challenge in Z_train:\n",
    "# #         if (list(challenge[64:66]) == [float(p), float(q)]):\n",
    "# #           trial_data1.append(challenge)\n",
    "# #         if (list(challenge[64:66]) == [float(q), float(p)]):\n",
    "# #           trial_data2.append(challenge)\n",
    "\n",
    "# #       trial_data1 = np.array(trial_data1)\n",
    "# #       trial_data2 = np.array(trial_data2)\n",
    "#         trial_data = []\n",
    "\n",
    "# #         for challenge in train_data:\n",
    "# #             if (list(challenge[64:66]) == [float(p), float(q)]):\n",
    "# #                 trial_data.append(challenge)\n",
    "# #             if (list(challenge[64:66]) == [float(q), float(p)]):\n",
    "# #                 check_challenge = challenge\n",
    "# #                 check_challenge[64:66] = [float(p), float(q)]\n",
    "# #                 a = [list(item) for item in trial_data]\n",
    "# #                 if list(check_challenge) not in a:\n",
    "# #                     challenge[-1] = 1.0 - challenge[-1]\n",
    "# #                     trial_data.append(challenge)\n",
    "\n",
    "\n",
    "#         for challenge in train_data:\n",
    "#             if (list(challenge[64:66]) == [float(q), float(p)]):\n",
    "#                 trial_data.append(challenge)\n",
    "#             if (list(challenge[64:66]) == [float(p), float(q)]):\n",
    "#                 check_challenge = challenge\n",
    "#                 check_challenge[64:66] = [float(q), float(p)]\n",
    "#                 a = [list(item) for item in trial_data]\n",
    "#                 if list(check_challenge) not in a:\n",
    "#                     challenge[-1] = 1.0 - challenge[-1]\n",
    "#                     trial_data.append(challenge)\n",
    "\n",
    "#         trial_data = np.array(trial_data)\n",
    "\n",
    "#         trial_data = np.delete(trial_data, [64, 65], axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         X = create_feature(trial_data[:, :-1])\n",
    "#         y = trial_data[:,-1]\n",
    "\n",
    "# #         sm = SMOTE(random_state=42)\n",
    "# #         X_res, y_res = sm.fit_resample(X, y)\n",
    "        \n",
    "# #         trial_data2 = np.delete(trial_data2, [64, 65], axis=1)\n",
    "# #         trial_data2[:,-1] = 1.0-trial_data2[:,-1]\n",
    "\n",
    "#         key = str(p)+'$'+str(q)\n",
    "\n",
    "#         model[key].fit(X, y)\n",
    "# #       model[key].fit(create_feature(trial_data2[:, :-1]), trial_data2[:,-1])\n",
    "def fit(model, Z_train):\n",
    "#     model = {}\n",
    "#     Z_train = transform_train_data(Z_train)\n",
    "\n",
    "    trn_data = {}\n",
    "\n",
    "    for challenge in Z_train:\n",
    "        p = int(challenge[64])\n",
    "        q = int(challenge[65])\n",
    "        key = None\n",
    "        if p<q:\n",
    "            key = str(p) + '$' + str(q)\n",
    "        else:\n",
    "            key = str(q) + '$' + str(p)\n",
    "\n",
    "        if trn_data.get(key) is None:\n",
    "            trn_data[key] = np.empty((0, 65), float)\n",
    "\n",
    "        challenge = np.delete(challenge, [64, 65])\n",
    "        if p<q:\n",
    "            trn_data[key] = np.append(trn_data[key], np.array([challenge]), axis=0)\n",
    "        else:\n",
    "            challenge[-1] = 1.0 - challenge[-1]\n",
    "            trn_data[key] = np.append(trn_data[key], np.array([challenge]), axis=0)\n",
    "\n",
    "    for key, data in trn_data.items():\n",
    "#         if model.get(key) is None:\n",
    "#             model[key] = LogisticRegression(C=100, max_iter=500)\n",
    "        X = create_feature(data[:, :-1])\n",
    "        y = data[:, -1]\n",
    "\n",
    "        model[key].fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "35GcjrNYH8E6"
   },
   "outputs": [],
   "source": [
    "def predict(model, Z_test):\n",
    "    predicted_response = []\n",
    "\n",
    "    for challenge in Z_test[:, :-1]:\n",
    "        p = int(challenge[64])\n",
    "        q = int(challenge[65])\n",
    "\n",
    "        challenge = np.delete(challenge, [64, 65])\n",
    "\n",
    "        if(p<q):\n",
    "            key = str(p)+'$'+str(q)\n",
    "            predicted_response.append((model[key].predict(create_feature([challenge])))[0])\n",
    "#             predicted_response.append(1.0-(model[key].predict(create_feature([challenge])))[0])\n",
    "        else:\n",
    "            key = str(q)+'$'+str(p)\n",
    "            predicted_response.append(1.0-(model[key].predict(create_feature([challenge])))[0])\n",
    "#             predicted_response.append((model[key].predict(create_feature([challenge])))[0])\n",
    "\n",
    "\n",
    "    predicted_response = np.array(predicted_response)\n",
    "    return predicted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2$3 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]]\n",
      "0$2 [[0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 0. ... 1. 0. 1.]\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n",
      "2$5 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [0. 1. 0. ... 0. 1. 0.]]\n",
      "1$11 [[0. 1. 0. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "1$9 [[1. 1. 1. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 0.]]\n",
      "4$15 [[0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "2$11 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]]\n",
      "6$8 [[1. 0. 1. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 1.]]\n",
      "8$11 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "6$9 [[1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "3$13 [[1. 1. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "12$15 [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 1. 1.]]\n",
      "1$10 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "3$5 [[1. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " [1. 0. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]]\n",
      "3$14 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 0. 1. 1.]\n",
      " [1. 0. 1. ... 0. 1. 1.]]\n",
      "2$9 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 1. 0. ... 0. 0. 1.]]\n",
      "7$13 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "0$4 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "1$4 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "7$8 [[0. 1. 1. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "0$1 [[0. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]]\n",
      "3$12 [[1. 1. 0. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 0. 0. 1.]]\n",
      "9$13 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 0. 1. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]]\n",
      "0$12 [[1. 1. 1. ... 0. 1. 1.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]]\n",
      "2$13 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "6$14 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 1. 1. 0.]]\n",
      "8$14 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [1. 0. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]]\n",
      "11$14 [[0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 1.]]\n",
      "1$14 [[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [1. 0. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "2$14 [[1. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 0.]]\n",
      "1$15 [[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]]\n",
      "6$10 [[1. 1. 1. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]]\n",
      "10$12 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 0. 1. 1.]]\n",
      "5$8 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 1. 1.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 1.]]\n",
      "0$13 [[0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "5$6 [[1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 0. 0.]]\n",
      "10$13 [[0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 1. 0.]]\n",
      "12$14 [[1. 1. 0. ... 1. 1. 0.]\n",
      " [1. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "3$10 [[0. 0. 1. ... 0. 1. 1.]\n",
      " [1. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 0. 1. 1.]]\n",
      "9$12 [[1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "8$13 [[0. 1. 0. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " [0. 1. 1. ... 1. 1. 0.]]\n",
      "3$11 [[1. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 1. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "8$12 [[1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]]\n",
      "5$13 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 0.]]\n",
      "4$12 [[0. 0. 1. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 1. 1. ... 0. 0. 1.]]\n",
      "2$8 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "0$9 [[1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 1. 1. 0.]]\n",
      "0$5 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]]\n",
      "2$7 [[1. 0. 1. ... 1. 1. 0.]\n",
      " [0. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 0.]]\n",
      "0$14 [[1. 1. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "1$12 [[1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "4$5 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "1$5 [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "13$15 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "1$2 [[0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 0. 0. 1.]]\n",
      "7$15 [[0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " [1. 0. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "4$9 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 1. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 1. 0. 1.]]\n",
      "1$7 [[0. 1. 0. ... 0. 1. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "10$11 [[1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 1. 1. 0.]]\n",
      "14$15 [[1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 0. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 1. 0.]\n",
      " [0. 1. 1. ... 1. 1. 1.]]\n",
      "2$4 [[1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "7$10 [[1. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "1$3 [[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 1. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "5$14 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " [1. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "0$8 [[0. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 1. ... 1. 1. 1.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "5$12 [[0. 0. 0. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "1$8 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 1. 0. 1.]]\n",
      "0$6 [[0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " [0. 1. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 0. 1. ... 1. 1. 1.]]\n",
      "6$7 [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 1. 1. ... 0. 1. 0.]]\n",
      "9$11 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 1.]]\n",
      "4$11 [[0. 0. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 1. 0. 0.]]\n",
      "6$12 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " [1. 0. 1. ... 1. 1. 1.]]\n",
      "11$12 [[1. 1. 1. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 1. 1. 1.]]\n",
      "9$15 [[0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "4$8 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 1. 1.]]\n",
      "1$6 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 0. 1. 1.]]\n",
      "3$7 [[1. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "4$10 [[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 0. 0. 1.]]\n",
      "7$14 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "6$15 [[0. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "11$13 [[1. 0. 1. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 0.]]\n",
      "3$4 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 1. 1. 0.]\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 1. 1. 0.]]\n",
      "4$6 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 0. 0. 1.]]\n",
      "0$11 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]]\n",
      "3$6 [[1. 1. 1. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [0. 1. 1. ... 1. 0. 1.]]\n",
      "5$15 [[0. 1. 1. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 1.]]\n",
      "5$9 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 1. 1. 1.]]\n",
      "2$15 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 1. 0. 1.]]\n",
      "8$10 [[0. 0. 0. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 0. 1. 0.]]\n",
      "2$12 [[0. 1. 1. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "5$7 [[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]]\n",
      "9$10 [[1. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 1. 1.]]\n",
      "3$8 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 0. ... 1. 1. 1.]]\n",
      "6$11 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "10$15 [[0. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "2$6 [[1. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 0. 0.]]\n",
      "13$14 [[0. 0. 1. ... 1. 1. 1.]\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]]\n",
      "4$13 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 0. 1.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "5$11 [[0. 1. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " [1. 0. 1. ... 1. 1. 0.]]\n",
      "7$11 [[1. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 0. 1. ... 0. 1. 1.]]\n",
      "7$12 [[1. 1. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "0$3 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 0. 0. 1.]\n",
      " [1. 0. 1. ... 1. 1. 1.]]\n",
      "0$15 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 0. 0. 1.]]\n",
      "6$13 [[0. 0. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "2$10 [[1. 0. 1. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]]\n",
      "9$14 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 1. 1. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "3$15 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]]\n",
      "7$9 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "12$13 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 0. 0.]]\n",
      "10$14 [[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "4$14 [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "5$10 [[1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 1. 0. 1.]]\n",
      "4$7 [[1. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 1. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "8$15 [[0. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 1. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " [0. 1. 1. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]]\n",
      "0$10 [[0. 1. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 1.]]\n",
      "3$9 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 0. 1.]\n",
      " [1. 0. 1. ... 0. 1. 1.]]\n",
      "11$15 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "8$9 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 1.]\n",
      " [1. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]]\n",
      "1$13 [[0. 0. 0. ... 1. 1. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 1. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 1. ... 1. 1. 0.]]\n",
      "0$7 [[1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 1. 0. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "    u = {}\n",
    "\n",
    "    for challenge in train_data:\n",
    "        p = int(challenge[64])\n",
    "        q = int(challenge[65])\n",
    "        key = None\n",
    "        if p<q:\n",
    "            key = str(p) + '$' + str(q)\n",
    "        else:\n",
    "            key = str(q) + '$' + str(p)\n",
    "\n",
    "        if u.get(key) is None:\n",
    "            u[key] = np.empty((0, 65), float)\n",
    "\n",
    "        challenge = np.delete(challenge, [64, 65])\n",
    "        if p<q:\n",
    "            u[key] = np.append(u[key], np.array([challenge]), axis=0)\n",
    "        else:\n",
    "            challenge[-1] = 1.0 - challenge[-1]\n",
    "            u[key] = np.append(u[key], np.array([challenge]), axis=0)\n",
    "\n",
    "    for key, data in u.items():\n",
    "        print(key, data)\n",
    "#         if model.get(key) is None:\n",
    "#             model[key] = LogisticRegression(C=100, max_iter=500)\n",
    "#         X = create_feature(data[:, :-1])\n",
    "#         y = data[:, -1]\n",
    "\n",
    "#         model[key].fit(X, y)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  2.,  3.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  2.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  2.,  5.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ..., 15.,  3.,  0.],\n",
       "       [ 0.,  0.,  1., ..., 13.,  7.,  1.],\n",
       "       [ 1.,  1.,  0., ..., 11., 14.,  1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "ll_rzWyvIE6J"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model1 = {}\n",
    "\n",
    "for p in range(0, 15):\n",
    "    for q in range(p+1, 16):\n",
    "        key = str(p)+'$'+str(q)\n",
    "        model1[key] = LinearSVC(loss = 'hinge', C=100, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTEIuf6jIJxY",
    "outputId": "c2f11a94-80b5-4b35-b496-19a89c6df3a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0$1': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$2': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$3': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$4': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$5': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$6': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$7': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '0$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$2': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$3': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$4': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$5': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$6': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$7': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '1$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$3': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$4': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$5': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$6': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$7': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '2$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$4': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$5': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$6': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$7': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '3$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$5': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$6': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$7': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '4$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$6': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$7': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '5$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$7': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '6$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$8': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '7$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '8$9': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '8$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '8$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '8$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '8$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '8$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '8$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '9$10': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '9$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '9$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '9$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '9$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '9$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '10$11': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '10$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '10$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '10$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '10$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '11$12': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '11$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '11$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '11$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '12$13': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '12$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '12$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '13$14': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '13$15': LinearSVC(C=100, loss='hinge', max_iter=10000),\n",
       " '14$15': LinearSVC(C=100, loss='hinge', max_iter=10000)}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model1, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "training_response = train_data[:, -1]\n",
    "print(training_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_training_response = predict(model1, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_training_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(np.average(predicted_training_response == training_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzRgKtKTIbUv",
    "outputId": "55e596fe-b7cf-4174-fe5d-a356bf752eb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_response = test_data[:, -1]\n",
    "actual_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykvodeYoIcBW",
    "outputId": "8e103029-f8eb-499d-e1f8-a4e2ceafe60d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_response1 = predict(model1, test_data)\n",
    "predicted_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzyXnPTUIeBE",
    "outputId": "b95bcf35-b231-4d16-89a5-e8d95d8ddaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061925\n"
     ]
    }
   ],
   "source": [
    "print(np.average(predicted_response1 == actual_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "AlfRE5QLI5qA"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model2 = {}\n",
    "\n",
    "for p in range(0, 15):\n",
    "    for q in range(p+1, 16):\n",
    "        key = str(p)+'$'+str(q)\n",
    "        model2[key] = LogisticRegression(C=100, max_iter=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "u41hh6tyKDnO"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, Z_train)\u001b[0m\n\u001b[0;32m     88\u001b[0m     X \u001b[38;5;241m=\u001b[39m create_feature(data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     89\u001b[0m     y \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\Anaconda3distribution\\envs\\gpu2\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1181\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1187\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1188\u001b[0m     )\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1191\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1.0"
     ]
    }
   ],
   "source": [
    "fit(model2, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_training_response2 = predict(model2, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.average(predicted_training_response2 == training_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRsjxxJWKNJU",
    "outputId": "e4b4e639-f032-4633-e630-8a01e5068061"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_response2 = predict(model2, test_data)\n",
    "predicted_response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzkuRfWiKX_F",
    "outputId": "576adb4d-1559-42fd-e734-174f6c7b124f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.949425\n"
     ]
    }
   ],
   "source": [
    "print(np.average(predicted_response2 == actual_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949425"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(actual_response, predicted_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_report = classification_report(actual_response, predicted_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.95      0.95     20200\n",
      "         1.0       0.95      0.95      0.95     19800\n",
      "\n",
      "    accuracy                           0.95     40000\n",
      "   macro avg       0.95      0.95      0.95     40000\n",
      "weighted avg       0.95      0.95      0.95     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model2_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19185,  1015],\n",
       "       [ 1008, 18792]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(actual_response, predicted_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision recall score: 0.93\n"
     ]
    }
   ],
   "source": [
    "model2_average_precision = average_precision_score(actual_response, predicted_response2)\n",
    "print('Average precision recall score: {:.2f}'.format(model2_average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "precision, recall, _ = precision_recall_curve(actual_response, predicted_response2)\n",
    "\n",
    "plt.step(recall, precision, alpha=1, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2)\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "# plt.xlim([0, 1.01])\n",
    "# plt.ylim([0, 0.04])\n",
    "plt.title('Model2 Precision Recall Curve: \\n Average Precision-Recall Score = {:.2f}'.format(model2_average_precision), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_fpr, model2_tpr, model2_threshold = roc_curve(training_response, predicted_training_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (125004746.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [72]\u001b[1;36m\u001b[0m\n\u001b[1;33m    # plt.show()\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('ROC Curve \\n 4 Best Performing Classifier', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(model2_fpr, model2_tpr, label='Logistic Regression Score: {:.3f}'.format(roc_auc_score(actual_response, predicted_training_response2))\n",
    "# plt.annotate('Minimum ROC score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.4), arrowprops=dict(facecolor='grey'))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "I4eBG8NHKfqK"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model3 = {}\n",
    "\n",
    "for p in range(0, 15):\n",
    "    for q in range(p+1, 16):\n",
    "        key = str(p)+'$'+str(q)\n",
    "        model3[key] = SVC(C=1000, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "xOgroniIK4y7"
   },
   "outputs": [],
   "source": [
    "fit(model3, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-mYZ8QDK8Nv",
    "outputId": "c6e1c2a0-bb55-45ea-cf9d-59a4114a7d23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_response3 = predict(model3, test_data)\n",
    "predicted_response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eI7bS0oLEFf",
    "outputId": "97cad3da-f9cc-4484-ad9a-6f1034aa93d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90535\n"
     ]
    }
   ],
   "source": [
    "print(np.average(predicted_response3 == actual_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "uBVVJfxnbVn9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model4 = {}\n",
    "\n",
    "for p in range(0, 15):\n",
    "    for q in range(p+1, 16):\n",
    "        key = str(p)+'$'+str(q)\n",
    "        model4[key] = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model4, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicted_response4 = predict(model4, test_data)\n",
    "# predicted_response4\n",
    "\n",
    "# #takes too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(predicted_response4 == actual_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "model5 = {}\n",
    "\n",
    "for p in range(0, 15):\n",
    "    for q in range(p+1, 16):\n",
    "        key = str(p)+'$'+str(q)\n",
    "        model5[key] = RidgeCV(alphas=np.logspace(-6, 6, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model5, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36726"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_response5 = predict(model5, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., -1., ...,  0.,  1.,  1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(predicted_response5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.average(predicted_response5 == actual_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model = LogisticRegression(C=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100, max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=100, max_iter=1000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_train = np.delete(train_data, [64, 65], axis=1)\n",
    "X = Z_train[:, :-1]\n",
    "y = Z_train[:,-1]\n",
    "\n",
    "single_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test = np.delete(test_data, [64, 65], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 65)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Z_test[:, :-1]\n",
    "y_test = Z_test[:,-1]\n",
    "\n",
    "y_pred = single_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, Z_train):\n",
    "  # for select bit p and q\n",
    "  for p in range(0, 15):\n",
    "    for q in range(p+1, 16):\n",
    "\n",
    "        trial_data = []\n",
    "\n",
    "        for challenge in train_data:\n",
    "            if (list(challenge[64:66]) == [float(p), float(q)]):\n",
    "                trial_data.append(challenge)\n",
    "            if (list(challenge[64:66]) == [float(q), float(p)]):\n",
    "                check_challenge = challenge\n",
    "                check_challenge[64:66] = [float(p), float(q)]\n",
    "                a = [list(item) for item in trial_data]\n",
    "                if list(check_challenge) not in a:\n",
    "                    challenge[-1] = 1.0 - challenge[-1]\n",
    "                    trial_data.append(challenge)\n",
    "\n",
    "        trial_data = np.array(trial_data)\n",
    "\n",
    "        trial_data = np.delete(trial_data, [64, 65], axis=1)\n",
    "        \n",
    "                \n",
    "        X = create_feature(trial_data[:, :-1])\n",
    "        y = trial_data[:,-1]\n",
    "\n",
    "\n",
    "        key = str(p)+'$'+str(q)\n",
    "\n",
    "        model[key].fit(X, y)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predicted_response = []\n",
    "\n",
    "    for challenge in Z_test[:, :-1]:\n",
    "        p = int(challenge[64])\n",
    "        q = int(challenge[65])\n",
    "\n",
    "        challenge = np.delete(challenge, [64, 65])\n",
    "\n",
    "        if(p<q):\n",
    "            key = str(p)+'$'+str(q)\n",
    "            predicted_response.append((model[key].predict(create_feature([challenge])))[0])\n",
    "        else:\n",
    "            key = str(q)+'$'+str(p)\n",
    "            predicted_response.append(1.0-(model[key].predict(create_feature([challenge])))[0])\n",
    "\n",
    "    predicted_response = np.array(predicted_response)\n",
    "    return predicted_response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
